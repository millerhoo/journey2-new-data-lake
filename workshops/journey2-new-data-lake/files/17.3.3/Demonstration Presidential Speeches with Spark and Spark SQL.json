{"paragraphs":[{"text":"%md\n# Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL\n\nThis tutorial was built for BDCS-CE version 17.3.3-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_new\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n**Contents**\n\n+ About Spark and Spark SQL\n+ Example: Spark(Scala) with Object Store data\n+ Example: Wordcount(Scala) \n+ Example: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n+ Example: Spark SQL against our Wordcounts table\n+ Example: More complex example- Speech Trends across time\n+ Example: Saving results to the Object Store\n+ Optional: Explore the Spark UI\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found: <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a>\n\n","dateUpdated":"2017-08-13T12:35:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Demonstration: Analyzing Presidential Speeches with Spark and Spark SQL</h1>\n<p>This tutorial was built for BDCS-CE version 17.3.3-20 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_new\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;\">&#x64;&#x61;&#118;&#105;d&#x2e;&#x62;a&#121;&#x61;&#x72;&#100;&#64;&#111;&#114;&#97;&#x63;&#x6c;&#101;&#46;&#99;&#111;&#x6d;</a></p>\n<p><strong>Contents</strong></p>\n<ul>\n  <li>About Spark and Spark SQL</li>\n  <li>Example: Spark(Scala) with Object Store data</li>\n  <li>Example: Wordcount(Scala)</li>\n  <li>Example: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</li>\n  <li>Example: Spark SQL against our Wordcounts table</li>\n  <li>Example: More complex example- Speech Trends across time</li>\n  <li>Example: Saving results to the Object Store</li>\n  <li>Optional: Explore the Spark UI</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found: <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774477_1719460640","id":"20170310-125455_1477278610","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:544","user":"anonymous","dateFinished":"2017-08-13T12:35:36+0000","dateStarted":"2017-08-13T12:35:36+0000"},{"text":"%md\n# About Spark and Spark SQL\n\nBDCS-CE version 17.3.3 comes with Spark version 2.1 and Scala version 2.11.  This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala) and Spark SQL.  This tutorial will give you examples using these.\n\nThe tutorial assumes you have a basic knowledge about Spark.  To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/2.1.0/quick-start.html\" target=\"_new\">Spark Quick Start</a>  and <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_new\">Spark SQL Programming Guide</a>\n\n\n","dateUpdated":"2017-08-13T12:36:08+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About Spark and Spark SQL</h1>\n<p>BDCS-CE version 17.3.3 comes with Spark version 2.1 and Scala version 2.11. This BDCS-CE version supplies Zeppelin interpreters for Spark(Scala) and Spark SQL. This tutorial will give you examples using these.</p>\n<p>The tutorial assumes you have a basic knowledge about Spark. To learn more about Spark, check out <a href=\"https://spark.apache.org/docs/2.1.0/quick-start.html\" target=\"_new\">Spark Quick Start</a> and <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_new\">Spark SQL Programming Guide</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774478_1720614887","id":"20170403-145735_495154166","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:545","user":"anonymous","dateFinished":"2017-08-13T12:36:08+0000","dateStarted":"2017-08-13T12:36:08+0000"},{"text":"%md\n# Setup - Downloading Presidental Speeches data\n\nNext, we will download some sample data to experiment with. In this example, we are going to download the text of a handful of United States Presidential Inauguration Speeches. We will install the lynx browser to help us. Then we will download the speeches from the Yale Law School Avalon Project website: <a href=\"http://avalon.law.yale.edu/subject_menus/inaug.asp\" target=\"_blank\">here</a>\n\n\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Setup - Downloading Presidental Speeches data</h1>\n<p>Next, we will download some sample data to experiment with. In this example, we are going to download the text of a handful of United States Presidential Inauguration Speeches. We will install the lynx browser to help us. Then we will download the speeches from the Yale Law School Avalon Project website: <a href=\"http://avalon.law.yale.edu/subject_menus/inaug.asp\" target=\"_blank\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774479_1720230138","id":"20170811-173348_755943744","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:546"},{"title":"Script to download sample speeches","text":"%sh\n\nCONTAINER=journeyC\nDIRECTORY=speeches\n\n#setup for this tutorial\necho \"running yum to install lynx. This may take 5 to 10 minutes to refresh yum cache. Please be patient\"\nsudo yum -y install lynx 2>&1\necho \"after yum\"\n\n#let's make the zeppelin directory readable.  This is so that Spark jobs can read these files later...\n#chmod ga+rx .\n\nmkdir speeches\ncd speeches\n\n#download some text from the internet using the lynx browser\n#we will grab inauguration speeches from here http://avalon.law.yale.edu/subject_menus/inaug.asp\n\nlynx -dump -nolist http://avalon.law.yale.edu/18th_century/wash1.asp | head -n -23 | tail -n +17 > pres1789_wash1.txt\nlynx -dump -nolist http://avalon.law.yale.edu/19th_century/lincoln1.asp | head -n -23 | tail -n +17 > pres1861_lincoln1.txt\nlynx -dump -nolist http://avalon.law.yale.edu/20th_century/froos1.asp | head -n -23 | tail -n +17 > pres1933_fdr1.txt\nlynx -dump -nolist http://avalon.law.yale.edu/20th_century/kennedy.asp | head -n -23 | tail -n +17 > pres1961_jfk.txt\nlynx -dump -nolist http://avalon.law.yale.edu/20th_century/reagan1.asp | head -n -23 | tail -n +17 > pres1981_reagan1.txt\n\necho \"files downloaded are:\"\nls -l\n\necho \"..\"\necho \"Copying files into Object Store\"\n\nhadoop fs -mkdir -p swift://$CONTAINER.default/$DIRECTORY/raw\nhadoop fs -put pres* swift://$CONTAINER.default/$DIRECTORY/raw/\nhadoop fs -ls swift://$CONTAINER.default/$DIRECTORY/raw\n\necho \"..\"\necho \"done\"\n","dateUpdated":"2017-08-13T12:37:08+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":"false"},"colWidth":12,"editorMode":"ace/mode/sh","title":true,"results":{},"enabled":true,"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774480_1730618358","id":"20170811-173803_1959686933","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:547","user":"anonymous","dateFinished":"2017-08-13T12:36:55+0000","dateStarted":"2017-08-13T12:36:22+0000"},{"text":"%md\n# Example: Spark(Scala) with Object Store data\n\nOur first example will show a simple bit of Scala code accessing our object store data.  Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in Object Store.  Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term. \n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Spark(Scala) with Object Store data</h1>\n<p>Our first example will show a simple bit of Scala code accessing our object store data. Our example defines a Spark RDD (Resilient Distributed Dataset) against a text file stored in Object Store. Then it runs a few actions against the RDD, such as counting the # of lines, displaying the first line, and counting the number of lines matching a given term.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774480_1730618358","id":"20170403-152657_222008853","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:548"},{"title":"Scala example with Object Store data","text":"%spark\n\nval Container=\"journeyC\"\nval Directory=\"speeches\"\n\nprintln(\"Define a RDD against a Object Store textfile...\")\nval textFile = sc.textFile(\"swift://\"+Container+\".default/\"+Directory+\"/raw/pres1981_reagan1.txt\")\n// Observe that we are specifying our file by the swift://ContainerName.ServiceName/ namespace.  The ServiceName (default in this example) points to set of configuration parameters.  The default configuration parameters were defined when you created this BDCS-CE instance. \n\n\nprintln(\"..\")\n\nprintln(\"Count of # of lines: %s\".format(\n    textFile.count() \n))    // Number of items in this RDD\nprintln(\"..\")\n\nprintln(\"The First Line: %s\".format(\n    textFile.first() // First item in this RDD\n))\nprintln(\"..\")\n\nprintln(\"# Lines containing the word Constitution: %s\".format(\n  textFile.filter(line => line.contains(\"Constitution\")).count() \n)) // How many lines contain \"Constitution\"?\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"2017-08-13T12:37:22+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774481_1730233610","id":"20170310-125552_378911250","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:549","user":"anonymous","dateFinished":"2017-08-13T12:37:26+0000","dateStarted":"2017-08-13T12:37:22+0000"},{"text":"%md\n# Example: Wordcount(Spark Scala) \n\nOur next example runs the classic Wordcount algorithm.  \n\nThe results of the Wordcount are an RDD named wordCounts that will also be used in the following example.\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Wordcount(Spark Scala)</h1>\n<p>Our next example runs the classic Wordcount algorithm. </p>\n<p>The results of the Wordcount are an RDD named wordCounts that will also be used in the following example.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774481_1730233610","id":"20170403-162628_622133907","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:550"},{"title":"Wordcount Example (scala)","text":"%spark\nval Container=\"journeyC\"\nval Directory=\"speeches\"\n\nprintln(\"Define a RDD against a Object Store textfile...\")\n\n// val textFile = sc.textFile(\"speeches_hdfs/pres1861_lincoln1.txt\")\n// val textFile = sc.textFile(\"file:/var/lib/zeppelin/speeches/pres1861_lincoln1.txt\")\nval textFile = sc.textFile(\"swift://\"+Container+\".default/\"+Directory+\"/raw/pres1861_lincoln1.txt\")\n\n// Now let's do the classic wordcount example with our data\n// if you are curious, the replaceAll() strips out punctuation and the split() separates the string into words.  Both use regex expressions.\nval wordCounts = textFile.flatMap(line => line.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase().split(\" +\")).map(word => (word, 1)).reduceByKey((a, b) => a + b)\n\nprintln(\"..\")\n\nprintln(\"Here is the output (partially shown)\")\nwordCounts.collect()\n\nprintln(\"\")\nprintln(\"..\")\n\nprintln(\"The first word is %s and the count of it is %s\".format(wordCounts.first()_1,wordCounts.first()_2))\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"2017-08-13T12:37:34+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774481_1731387856","id":"20170310-125659_767950724","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:551","user":"anonymous","dateFinished":"2017-08-13T12:37:39+0000","dateStarted":"2017-08-13T12:37:34+0000"},{"text":"%md\n# Example: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)\n\nThe next example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame.  Then it registers the new data frame as a temporary Spark SQL table.\n\nAt this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">here</a>\n\n","dateUpdated":"2017-08-13T12:38:07+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Converting wordCounts RDD to a DataFrame and registering it as table (Scala)</h1>\n<p>The next example continues from the previous example. Specifically, it takes the wordCounts RDD and registers it as Spark DataFrame. Then it registers the new data frame as a temporary Spark SQL table.</p>\n<p>At this point, you might want to quickly review the Spark SQL programming guide for a refresher about Creating DataFrames from RDDs: <a href=\"https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\" target=\"_blank\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774482_1731387856","id":"20170403-163207_132564105","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:552","user":"anonymous","dateFinished":"2017-08-13T12:38:07+0000","dateStarted":"2017-08-13T12:38:07+0000"},{"title":"Converting a RDD to a DataFrame and Registering it as Table (scala)","text":"%spark\n\n// this example follows the \"Inferring the Schema using Reflection\" of converting an RDD to a DataFrame. See https://spark.apache.org/docs/2.1.0/sql-programming-guide.html\n\n// first define a case class that describes our Schema \ncase class Wordcount(word: String, wcount: Int)\nprintln(\"..\")\n\n// second, map our wordCounts RDD into an RDD using our Wordcount class and convert that into a DataFrame\nval wordcountsDF = wordCounts.map(p => Wordcount(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// now register our wordcountsDF data frame as a temporary Spark SQL table\nwordcountsDF.registerTempTable(\"wordcounts\")\nprintln(\"..\")\n\nprintln(\"done\")","dateUpdated":"2017-08-13T12:37:48+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774482_1731387856","id":"20170314-135312_277685101","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:553","user":"anonymous","dateFinished":"2017-08-13T12:37:50+0000","dateStarted":"2017-08-13T12:37:49+0000"},{"text":"%md\n# Example: Spark SQL against our wordcounts table\n\nThis example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table.  It also demonstrates some of the features of Zeppelin's Spark SQL interpreter and display visualization capabilities.\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Spark SQL against our wordcounts table</h1>\n<p>This example continues from the previous example. Specifically, it provides two examples of running Spark SQL against our wordcounts table. It also demonstrates some of the features of Zeppelin&rsquo;s Spark SQL interpreter and display visualization capabilities.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774483_1731003107","id":"20170403-164432_219780017","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:554"},{"title":"Spark SQL against our WordCounts table","text":"%sql\nselect * from wordcounts \nwhere length(word)>4\norder by wcount desc limit 15","dateUpdated":"2017-08-13T12:38:13+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774483_1731003107","id":"20170314-140802_470559038","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:555","user":"anonymous","dateFinished":"2017-08-13T12:38:14+0000","dateStarted":"2017-08-13T12:38:14+0000"},{"title":"More Spark SQL","text":"%sql\nselect * from wordcounts\nwhere word in ('god','america','constitution','nation','people','states','rights','freedom','union')\norder by wcount desc","dateUpdated":"2017-08-12T19:56:14+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"wcount","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"wcount","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774484_1729079363","id":"20170314-140955_1723818486","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:556"},{"text":"%md\n# Example: More complex example- Speech Trends across time\n\nOur next example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store.  Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table.  Then we use some SparkSQL to prepare a couple of charts.\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: More complex example- Speech Trends across time</h1>\n<p>Our next example continues from the previous example. Specifically, it builds an RDD against all of the speeches we stored in the Object Store. Then it performs a word count against the RDD and converts the result into a Data Frame that we register as a Spark SQL temporary table. Then we use some SparkSQL to prepare a couple of charts.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774484_1729079363","id":"20170403-165335_535102146","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:557"},{"text":"%spark\n// more complex example.  Let's look for trends across presidents\n\nval Container=\"journeyC\"\nval Directory=\"speeches\"\n\nval filebase = \"swift://\"+Container+\".default/\"+Directory+\"/raw/pres\"\n\nval textFiles = sc.wholeTextFiles(filebase+\"*.txt\")\n\nprintln(\"We found %s speeches\".format(textFiles.count()))\nprintln(\"..\")\n\n// this cleans up the filename to make it shorter and removes some punctuation from the text\nval words = textFiles.map(file => (file._1.replace(filebase,\"\"),file._2.replaceAll(\"[^\\\\w ]\",\" \").toLowerCase()))\nprintln(\"..\")\n\n// this splits the String of text into an array of words/Strings\nval wordArray = words.map(file => (file._1,file._2.split(\" +\")))\nprintln(\"..\")\n\n// define our class/schema\ncase class filewordsraw(file: String, words: Array[String])\nprintln(\"..\")\n\n// create a DataFrame from our RDD\nval filewordsrawDF = wordArray.map(p => filewordsraw(p._1, p._2)).toDF()\nprintln(\"..\")\n\n// show the schema\nfilewordsrawDF.printSchema()\nprintln(\"..\")\n\n// register this as a table (we'll do the rest of the word count in SQL)\nfilewordsrawDF.registerTempTable(\"filewordsraw\")\nprintln(\"..\")\n\nprintln(\"done\")\n","dateUpdated":"2017-08-13T12:38:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774485_1728694614","id":"20170314-161337_919830878","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:558","user":"anonymous","dateFinished":"2017-08-13T12:38:26+0000","dateStarted":"2017-08-13T12:38:21+0000"},{"title":"SQL Showing the # of words per Speech","text":"%sql\nselect file, count(*) words from (select file, explode(words) word from filewordsraw) a\ngroup by file\norder by file","dateUpdated":"2017-08-12T19:56:14+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"words","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}},"forceY":true}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774485_1728694614","id":"20170403-165913_699538396","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:559"},{"title":"Chart showing the usage of certain words over time","text":"%sql\nselect file, word, count(*) wc from (select file, explode(words) word from filewordsraw) a\nwhere word in ('god','america','constitution','nation','rights','freedom','people')\ngroup by file, word\norder by file","dateUpdated":"2017-08-13T12:38:33+0000","config":{"editorSetting":{"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"keys":[{"name":"file","index":0,"aggr":"sum"}],"values":[{"name":"wc","index":2,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"file","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774486_1729848861","id":"20170314-173832_82214833","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:560","user":"anonymous","dateFinished":"2017-08-13T12:38:35+0000","dateStarted":"2017-08-13T12:38:33+0000"},{"text":"%md\n# Example: Saving a Data Frame back to the Object Store\n\nOur final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file).  Then it reads the json data back from the Object Store into a new Data Frame.\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Example: Saving a Data Frame back to the Object Store</h1>\n<p>Our final example continues from the previous example. Specifically, it defines a new data frame based off one of the sample SQL statements and writes that data frame back to the Object Store (as a json file). Then it reads the json data back from the Object Store into a new Data Frame.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774486_1729848861","id":"20170403-200206_1232128960","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:561"},{"text":"%spark\n\nval Container=\"journeyC\"\nval Directory=\"speeches\"\n\nval filebase = \"swift://\"+Container+\".default/\"+Directory+\"/processed/wordcount_json\"\n\n\n// \nval results = sqlContext.sql(\"select file, count(*) words from (select file, explode(words) word from filewordsraw) a group by file\")\nprintln(\"..\")\n\n\nprintln(\"..\")\n\n// save in json format.  the repartition(1) ensures that we write a single output file, which makes sense since we know the output is small\nresults.repartition(1).write.format(\"json\").mode(\"overwrite\").save(filebase)\nprintln(\"..\")\n\n// now lets read it back\nval df = sqlContext.read.format(\"json\").load(filebase)\nprintln(\"..\")\n\ndf.collect()\nprintln(\"..\")\n\nprintln(\"done\")\n\n","dateUpdated":"2017-08-13T12:38:45+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774486_1729848861","id":"20170403-200515_1585180633","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:562","user":"anonymous","dateFinished":"2017-08-13T12:39:04+0000","dateStarted":"2017-08-13T12:38:45+0000"},{"title":"Optional: List the contents of your Object Store container to see the structure of the saved data frame.","text":"%sh\nCONTAINER=journeyC\nDIRECTORY=speeches\nhadoop fs -ls swift://$CONTAINER.default/$DIRECTORY/processed/wordcount_json\n","dateUpdated":"2017-08-13T12:39:53+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"container":"demo"},"forms":{}},"apps":[],"jobName":"paragraph_1502567774487_1729464112","id":"20170403-203400_319359235","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:563","user":"anonymous","dateFinished":"2017-08-13T12:39:43+0000","dateStarted":"2017-08-13T12:39:37+0000"},{"text":"%md\n# Optional: Explore the Spark UI\n\nWhen you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server.  In many situations, it can be helpful to view the \"Spark UI\" for your session.  BDCS-CE provides easy access to Spark UI for your Zeppelin session.\n\nTo view it, follow these steps...\n\n + Click on the Jobs tab\n + Find the running (\"Processing\") Zeppelin job\n + From the pop-up menu, choose Spark UI\n![spark ui](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg \"Spark UI\")\n + Click on Attempt_1\n + Explore the Spark UI\n![SparkUI](https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg \"Spark UI\")\n\n\n\n\n\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Optional: Explore the Spark UI</h1>\n<p>When you use Spark (via Scala, Python, and/or SQL), you start a session with the Spark server. In many situations, it can be helpful to view the &ldquo;Spark UI&rdquo; for your session. BDCS-CE provides easy access to Spark UI for your Zeppelin session.</p>\n<p>To view it, follow these steps&hellip;</p>\n<ul>\n  <li>Click on the Jobs tab</li>\n  <li>Find the running (&ldquo;Processing&rdquo;) Zeppelin job</li>\n  <li>From the pop-up menu, choose Spark UI<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011441.jpg\" alt=\"spark ui\" title=\"Spark UI\" /></li>\n  <li>Click on Attempt_1</li>\n  <li>Explore the Spark UI<br/><img src=\"https://raw.githubusercontent.com/oracle/learning-library/master/workshops/journey2-new-data-lake/images/300/snap0011442.jpg\" alt=\"SparkUI\" title=\"Spark UI\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774487_1729464112","id":"20170405-085653_1479537842","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:564"},{"text":"%md\n# Next Steps\n\n+ Review the Spark and Spark SQL documentation here: <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\" target=\"_new\">Spark Quick Start</a>  and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\" target=\"_new\">Spark SQL Programming Guide</a>\n+ Experiment with your own data sets\n+ Proceed to one of the other tutorials","dateUpdated":"2017-08-12T19:56:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Next Steps</h1>\n<ul>\n  <li>Review the Spark and Spark SQL documentation here: <a href=\"https://spark.apache.org/docs/1.6.1/quick-start.html\" target=\"_new\">Spark Quick Start</a> and <a href=\"https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\" target=\"_new\">Spark SQL Programming Guide</a></li>\n  <li>Experiment with your own data sets</li>\n  <li>Proceed to one of the other tutorials</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774488_1727540367","id":"20170310-130044_26976236","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:565"},{"text":"%md\n### Change Log\nAugust 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20\nAugust 11, 2017 - Journey v2. Confirmed it works with Spark 2.1\nJuly 28, 2017 - Updated to work with BDCSCE 17.3.1-20","dateUpdated":"2017-08-13T12:39:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>August 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20<br/>August 11, 2017 - Journey v2. Confirmed it works with Spark 2.1<br/>July 28, 2017 - Updated to work with BDCSCE 17.3.1-20</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1502567774488_1727540367","id":"20170424-170410_551484051","dateCreated":"2017-08-12T19:56:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:566","user":"anonymous","dateFinished":"2017-08-13T12:39:31+0000","dateStarted":"2017-08-13T12:39:31+0000"},{"text":"%md\n","dateUpdated":"2017-08-12T19:56:14+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1502567774489_1727155618","id":"20170728-141213_1235978622","dateCreated":"2017-08-12T19:56:14+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:567"}],"name":"Demonstration Presidential Speeches with Spark and Spark SQL","id":"2CQZSD6MK","angularObjects":{"2CQA2VQ3C:shared_process":[],"2CSX451P1:shared_process":[],"2CQH9FYAV:shared_process":[],"2CPQ7W1ZD:shared_process":[],"2CQDB4M3A:shared_process":[],"2CSD8BUF1:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CRZ9M6JB:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}