{"paragraphs":[{"text":"%md\n# Citi Bike Live Map Demonstration\n\nThis tutorial was built for BDCS-CE version 17.4.1 and OEHCS 0.10 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>.  Questions and feedback about the tutorial: <david.bayard@oracle.com>\n\n    NOTE: Please ensure that you have run the \"Working with Spark Interpreter\" and \"Working with OEHCS and Spark Streaming\" Tutorials first.\n\n**Contents**\n\n+ OEHCS Setup\n+ Preparing Bike Data for Streaming\n+ Writing a Producer to stream data to OEHCS\n+ Running the Live Map demonstration\n+ Next Steps\n\n\nAs a reminder, the documentation for BDCS-CE can be found  <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a>\n\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Citi Bike Live Map Demonstration</h1>\n<p>This tutorial was built for BDCS-CE version 17.4.1 and OEHCS 0.10 as part of the New Data Lake User Journey: <a href=\"https://github.com/oracle/learning-library/tree/master/workshops/journey2-new-data-lake\" target=\"_blank\">here</a>. Questions and feedback about the tutorial: <a href=\"mailto:&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;\">&#100;&#97;&#118;&#x69;&#100;&#46;&#x62;&#97;&#x79;&#x61;&#x72;&#x64;&#64;&#x6f;&#114;&#97;c&#108;&#x65;&#x2e;c&#x6f;&#109;</a></p>\n<pre><code>NOTE: Please ensure that you have run the &quot;Working with Spark Interpreter&quot; and &quot;Working with OEHCS and Spark Streaming&quot; Tutorials first.\n</code></pre>\n<p><strong>Contents</strong></p>\n<ul>\n  <li>OEHCS Setup</li>\n  <li>Preparing Bike Data for Streaming</li>\n  <li>Writing a Producer to stream data to OEHCS</li>\n  <li>Running the Live Map demonstration</li>\n  <li>Next Steps</li>\n</ul>\n<p>As a reminder, the documentation for BDCS-CE can be found <a href=\"https://docs.oracle.com/cloud/latest/big-data-compute-cloud/index.html\" target=\"_new\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379847_-388689070","id":"20170417-194925_2070687465","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2322"},{"text":"%md\n# OEHCS Setup\n\nThis demonstration will use Oracle Event Hub Cloud Service (OEHCS) and Spark Streaming.  Be sure that you have completed the OEHCS tutorial \"Working with OEHCS and Spark Streaming\" before running this demonstration.  That tutorial will help you setup connectivity to OEHCS and create a Kafka topic.  You will need to enter the OEHCS Connection Descriptor and OEHCS Topic in the paragraph below and run it.","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>OEHCS Setup</h1>\n<p>This demonstration will use Oracle Event Hub Cloud Service (OEHCS) and Spark Streaming. Be sure that you have completed the OEHCS tutorial &ldquo;Working with OEHCS and Spark Streaming&rdquo; before running this demonstration. That tutorial will help you setup connectivity to OEHCS and create a Kafka topic. You will need to enter the OEHCS Connection Descriptor and OEHCS Topic in the paragraph below and run it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379847_-388689070","id":"20170503-140655_2083705382","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2323"},{"title":"Parameters (be sure to run this)","text":"%spark\nz.angularBind(\"BIND_ObjectStorage_Container\", \"journeyC\")\nz.angularBind(\"BIND_OEHCS_ConnectionDescriptor\", z.input(\"OEHCS_ConnectionDescriptor\",\"141.144.144.128:6667\"))\nz.angularBind(\"BIND_OEHCS_Topic\", z.input(\"OEHCS_Topic\",\"gse00010212-TutorialOEHCS\"))\n\n                         \nscala.tools.nsc.io.File(\"/var/lib/zeppelin/bikes_part3.sh\").writeAll(\n  \"export ObjectStorage_Container=\\\"\"+z.angular(\"BIND_ObjectStorage_Container\")+\"\\\"\\n\" +\n  \"export OEHCS_ConnectionDescriptor=\\\"\"+z.angular(\"BIND_OEHCS_ConnectionDescriptor\")+\"\\\"\\n\" +\n  \"export OEHCS_Topic=\\\"\"+z.angular(\"BIND_OEHCS_Topic\")+\"\\\"\\n\"\n)\nprintln(\"done\")\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{"Top N":"10","OEHCS_Topic":"gse00002281-TutorialOEHCS","dayOfWeek":"All","ObjectStorage_Container":"citibike","OEHCS_ConnectionDescriptor":"140.86.32.89:6667","hourOfDay":"All","David":"rocks"},"forms":{"OEHCS_Topic":{"name":"OEHCS_Topic","displayName":"OEHCS_Topic","type":"input","defaultValue":"gse00010212-TutorialOEHCS","hidden":false,"$$hashKey":"object:2788"},"OEHCS_ConnectionDescriptor":{"name":"OEHCS_ConnectionDescriptor","displayName":"OEHCS_ConnectionDescriptor","type":"input","defaultValue":"141.144.144.128:6667","hidden":false,"$$hashKey":"object:2787"}}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"done\n"}]},"apps":[],"jobName":"paragraph_1510931379848_-390612815","id":"20170426-101707_919098031","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2324"},{"text":"%md\n# Preparing Bike Data for Streaming\n\nWe will take our bike trip data and wrangle it into a format that will be easier to use with streaming.  Specifically, our current data provides 1 row of data which has both the start and end times of a bike trip.  We will wrangle the data into a new file such that the start of the trip is its own row and the end of the trip is also its own row.  And we will sort our new file by the appropriate event time for that row (start or end).  This will make it easier for our Kafka producer program to stream the data for us.  We will use Spark SQL to make the wrangling easy.  Finally, we'll write our new data to a container in the Object Store.\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Preparing Bike Data for Streaming</h1>\n<p>We will take our bike trip data and wrangle it into a format that will be easier to use with streaming. Specifically, our current data provides 1 row of data which has both the start and end times of a bike trip. We will wrangle the data into a new file such that the start of the trip is its own row and the end of the trip is also its own row. And we will sort our new file by the appropriate event time for that row (start or end). This will make it easier for our Kafka producer program to stream the data for us. We will use Spark SQL to make the wrangling easy. Finally, we&rsquo;ll write our new data to a container in the Object Store.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379850_-389843317","id":"20170421-162727_412866557","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2325"},{"title":"Spark code to wrangle raw data into a streaming input dataset","text":"%spark\n\n\n//a previous tutorial placed the csv file into your Object Store citibike container\n//notice the use of the swift://CONTAINER.defaut/ syntax\n\nval df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(\"swift://\"+z.angular(\"BIND_ObjectStorage_Container\")+\".default/citibike/raw/201612-citibike-tripdata.csv\")\n\n//cache the data frame for performance\ndf.cache()\n\n\nprintln(\"Here is the schema detected from the CSV\")\ndf.printSchema()\nprintln(\"..\")\n\nprintln(\"# of rows: %s\".format(\n  df.count() \n)) \nprintln(\"..\")\n\ndf.createOrReplaceTempView(\"bike_trips_temp\")\n\n\nprintln(\"Wrangling the existing data into a new dataframe\")\n// create a new DataFrame that creates separate rows for start and end events\nval df = sqlContext.sql(s\"\"\"select b.`Start Time` EventTime, \"Pickup\" EventType, \n    case when b.gender=1 then 'Male' when b.gender=2 then 'Female' else 'unknown' end GenderStr, b.* from bike_trips_temp b\nunion all\nselect b.`Stop Time` EventTime, \"Dropoff\" EventType, \n    case when b.gender=1 then 'Male' when b.gender=2 then 'Female' else 'unknown' end GenderStr, b.* from bike_trips_temp b\n\"\"\")\n\nprintln(\"Writing new data to Object Store.  This may take 5 minutes.. Please be patient.  If bored, you can explore the running status via the Spark UI.\")\n//write the new data frame out as a csv file\ndf.repartition(1).sortWithinPartitions(\"EventTime\").write.format(\"com.databricks.spark.csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"swift://\"+z.angular(\"BIND_ObjectStorage_Container\")+\".default/citibike/scratch/bike_streaming_input\")\n\nprintln(\"done\")\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":494,"optionOpen":false}}},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379851_-390228066","id":"20170421-163037_1592342947","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2326"},{"title":"Shell script to get a local copy of our new streaming input datafile","text":"%sh\n# This script copies the new dataset we created to the local file system, where our producer script is expecting it to reside.\n\n. bikes_part3.sh\necho Object Store Container = $ObjectStorage_Container\n\n\nhadoop fs -ls swift://$ObjectStorage_Container.default/citibike/scratch/bike_streaming_input \n\ncd citibike\nrm bike_streaming_input.csv\n\nhadoop fs -get swift://$ObjectStorage_Container.default/citibike/scratch/bike_streaming_input/part-00000* bike_streaming_input.csv \n\nls -l bike_streaming_input.csv\nhead bike_streaming_input.csv\n\necho \"done\"\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379851_-390228066","id":"20170421-171627_570636554","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2327"},{"text":"%md\n# Writing a Producer to stream data to OEHCS\n\nFor this demonstration, we will replay historical bike pickup and dropoff data back in \"real-time\".  In other words, if we start the clock at 8am and our first bike is picked up at 8:00:14, our producer program will send the pickup event 14 seconds after it is ready to begin.  If the next bike event is at 8:00:25, the producer will wait 11 more seconds before sending that event.  We also have a \"time acceleration\" parameter we can use to speed up how fast our producer replays the historical data.\n\nOur producer program is written in python.  It will stream to OEHCS via the \"kafka-python\" library, as described <a href=\"https://github.com/dpkp/kafka-python\" target=\"_blank\">here</a>\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Writing a Producer to stream data to OEHCS</h1>\n<p>For this demonstration, we will replay historical bike pickup and dropoff data back in &ldquo;real-time&rdquo;. In other words, if we start the clock at 8am and our first bike is picked up at 8:00:14, our producer program will send the pickup event 14 seconds after it is ready to begin. If the next bike event is at 8:00:25, the producer will wait 11 more seconds before sending that event. We also have a &ldquo;time acceleration&rdquo; parameter we can use to speed up how fast our producer replays the historical data.</p>\n<p>Our producer program is written in python. It will stream to OEHCS via the &ldquo;kafka-python&rdquo; library, as described <a href=\"https://github.com/dpkp/kafka-python\" target=\"_blank\">here</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379852_-392151810","id":"20170421-163756_1232378663","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2328"},{"title":"Shell command to save our Python program to a script","text":"%sh\n. bikes_part3.sh\n\necho \"\n#!/usr/bin/env python\n\n# standard libraries\nimport os\nimport sys\nimport csv\nimport json\nfrom time import sleep\nimport datetime as dt\n\n# Quality of Life Utils\nimport dateutil.parser\n\n# kafka-python libraries\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\n\nif len(sys.argv) < 6:\n\tprint 'usage: tutorial_kafka.py [inputfile:/path/to/file.csv] [acceleration-factor:integer] [recordcount:int-0 is infinite] [starttime:YYYY-MM-DD hh:mm:ss]'\n\tsys.exit(1)\n\ninputfile = sys.argv[1]\naccelerator = float(sys.argv[2])\nrecordcount = int(sys.argv[3])\ninputstarttime = dateutil.parser.parse(' '.join(sys.argv[4:]))\n\nscriptstarttime = dt.datetime.now()\nrelativestarttimedelta = inputstarttime - scriptstarttime\n\ndef timedelta_total_seconds(timedelta):\n    return (\n        timedelta.microseconds + 0.0 +\n        (timedelta.seconds + timedelta.days * 24 * 3600) * 10 ** 6) / 10 ** 6\n\n\n# When this baby hits 88mph, you're going to see some serious stuff.\ndef delorean(accelerator, scriptstarttime):\n\tnowtime = dt.datetime.now()\n\treturn scriptstarttime + dt.timedelta(seconds=accelerator*timedelta_total_seconds(nowtime - scriptstarttime))\n\n\n# Kafka Stuff\n# put your broker hostname:port in single quotes inside those bracketse\nmykafkaservers = ['$OEHCS_ConnectionDescriptor']\nproducer = KafkaProducer(bootstrap_servers=mykafkaservers, value_serializer=lambda m: json.dumps(m).encode('utf-8'))\n\n\ni = 0\n# Reader Loop\nwith open(inputfile) as csvfile:\n\treader = csv.DictReader(csvfile)\n\tfor rec in reader:\n\t\teventtime = dateutil.parser.parse(rec['EventTime'])\n\t\tif eventtime < inputstarttime:\n\t\t\tcontinue\n\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\twhile (relativenowtime < eventtime):\n\t\t\trelativenowtime = delorean(accelerator, scriptstarttime) + relativestarttimedelta\n\t\t\t# print relativenowtime, eventtime\n\t\t\tsleep(0.1)\n\t\t#print str(i)+'/'+str(recordcount)+' sending: ', rec\n\t\tprint str(i)+'/'+str(recordcount)\n\t\tproducer.send('$OEHCS_Topic', rec)\n\t\ti += 1\n\t\tif i >= recordcount and recordcount != 0:\n\t\t\tbreak\n\" > citibike_kafka.py\n\nls -l citibike_kafka.py\necho \"done\"","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{"OEHCS_Topic":"gse00010212-TutorialOEHCS","OEHCS_ConnectionDescriptor":"141.144.144.128:6667"},"forms":{}},"apps":[],"jobName":"paragraph_1510931379852_-392151810","id":"20170421-163807_533257721","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2329"},{"text":"%md\n#Running the Live Map demonstration\n\nAssuming you have run the above paragraphs, we are now ready to run our Live Map demonstration.  This demonstration will show a live map showing the latest pickups and dropoffs.  Pickups will be shown with green markers with a green line indicating the direction where the bike will eventually be dropped off (since we are replaying historical data, we conviently know the final dropoff location!).  Dropoff are shown in red with the line indicating where the bike came from.  Longer lines indicate longer trips.\n\n\nTo do run the demo,\n + **Run the Spark Streaming paragraph below**.  This will start a Spark Streaming session with a 5 second window.  As new data arrives, it will update the map (below).  This session will run for a few minutes before stopping itself.\n + Be sure to **also run the Producer paragraph below** otherwise there will be no data for the Spark Streaming session to see.  \n + With both the Spark Streaming and Producer running, **watch the output of those paragraphs as well as the map paragraph** to see changes.\n\n \nThe code editors for the next three paragraphs are hidden by default to make it simpler to run, but definitely show the code editors to see how the code is written.\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Running the Live Map demonstration</h1>\n<p>Assuming you have run the above paragraphs, we are now ready to run our Live Map demonstration.  This demonstration will show a live map showing the latest pickups and dropoffs.  Pickups will be shown with green markers with a green line indicating the direction where the bike will eventually be dropped off (since we are replaying historical data, we conviently know the final dropoff location!).  Dropoff are shown in red with the line indicating where the bike came from.  Longer lines indicate longer trips.</p>\n<p>To do run the demo,</p>\n<ul>\n<li><strong>Run the Spark Streaming paragraph below</strong>.  This will start a Spark Streaming session with a 5 second window.  As new data arrives, it will update the map (below).  This session will run for a few minutes before stopping itself.</li>\n<li>Be sure to <strong>also run the Producer paragraph below</strong> otherwise there will be no data for the Spark Streaming session to see.</li>\n<li>With both the Spark Streaming and Producer running, <strong>watch the output of those paragraphs as well as the map paragraph</strong> to see changes.</li>\n</ul>\n<p>The code editors for the next three paragraphs are hidden by default to make it simpler to run, but definitely show the code editors to see how the code is written.</p>\n"}]},"apps":[],"jobName":"paragraph_1510931379853_-392536559","id":"20170421-164116_456910782","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2330"},{"title":"Spark Streaming Live Map code","text":"%spark\n// Be sure to have run Part 2 of this tutorial first (and in the same zeppelin spark session).  This code assumes that bike_trips from Part 2 is a registered Spark SQL table\n\n//Define a class for the structure of the data we will be passing to the map javascript code\ncase class BikeEvents(eventtype: String, station: String, description: String, lat: Double, lon: Double, midwaylat: Double, midwaylon: Double)\n  //TODO: what other fields?  \n  // maybe midwaylat,lon (for lines)\n  //want to also keep startlat, startlon, endlat, endlon\n\nz.angularUnbind(\"bikeevents\")\n\n// println(\"topic:\"+z.angular(\"OEHCS_Topic\"))\n{\n    \n\nimport _root_.kafka.serializer.StringDecoder //http://stackoverflow.com/questions/36397688/sbt-cannot-import-kafka-encoder-decoder-classes\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\n\n\n\n println(\"Creating new Streaming Context\")\n val ssc = new StreamingContext(sc, Seconds(5))\n \n\n val topic = z.angular(\"BIND_OEHCS_Topic\").toString\n println(\"topic:\"+topic)\n val topicsSet = topic.split(\",\").toSet\n \n val brokers=z.angular(\"BIND_OEHCS_ConnectionDescriptor\").toString\n println(\"brokers:\"+brokers)\n val kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\n \n println(\"Creating Kafka DStream\")\n //https://spark.apache.org/docs/1.6.1/streaming-kafka-integration.html\n val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n      ssc, kafkaParams, topicsSet)\n\n\n\n println(\"Setting up operations on DStream\")    \n \n //for debugging, you can print the full contents of the first 10 rows of each batch of messages by uncommenting the following\n //messages.print()\n \n \n messages.foreachRDD(rdd => {\n     //our Kafka data comes in Key,Value format.  we only care about the value, so use a map to extract just the 2nd element\n     var values=rdd.map(kv => kv._2)\n     \n     //for this example, the value is a JSON string.  Let's make a DataFrame out of the JSON strings\n     var df=sqlContext.read.json(values)\n     \n     var reccount = df.count()\n     //let's print out the count\n     printf(\"count = %s \\n\",reccount)\n     \n     //check to see if we have any rows...\n     if (reccount >0)  {\n        //let's print the first row\n        //println(\"first row \",df.first())\n    \n        \n        //Map the DF into an Array of BikeEvents. \n        var eventStr = if (\"EventType\"==\"Pickup\") \"Start\" else \"End\"\n        \n        var items = df.map(b => \n            BikeEvents(b.getAs[String](\"EventType\"), b.getAs[String](eventStr +\" Station Name\"), \"Gender: \"+ b.getAs[String](\"GenderStr\") + \" Birth Year:\"+b.getAs[String](\"Birth Year\"), b.getAs[String](eventStr +\" Station Latitude\").toDouble, b.getAs[String](eventStr +\" Station Longitude\").toDouble,\n            (b.getAs[String](\"Start Station Latitude\").toDouble+b.getAs[String](\"End Station Latitude\").toDouble)/2, (b.getAs[String](\"Start Station Longitude\").toDouble+b.getAs[String](\"End Station Longitude\").toDouble)/2)\n        )\n        //println(\"first items row \",items.first())\n           \n        //Bind the BikeEvents to an Angular UI variable named bikeevents\n        z.angularBind(\"bikeevents\", items.collect()) \n     } \n     \n })\n \n println(\"Starting Streaming Context\")\n ssc.start()\n\n \n println(\"Will now sleep for a few minutes, before stopping the StreamingContext\")\n\n //now sleep for a few minutes.  Parameter is milliseconds\n Thread.sleep(120000)\n\n //stop any active streamingcontexts.  Parameters are boolean stopSparkContext, boolean stopGracefully\n println(\"Stopping Active StreamingContext\")\n StreamingContext.getActive().map(_.stop(false,true))\n\n println(\"done\")\n\n} \n\n//hint: if the code does not appear to work but you don't see any errors, check the zeppelin spark log file at\n///u01/bdcsce/data/var/log/zeppelin/zeppelin-interpreter-spark-zeppelin-*.log","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379853_-392536559","id":"20170421-164148_1484226247","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2331"},{"title":"Producer for the Live Map","text":"%sh\n#The arguments are datafile TimeSpeedupFactor RecordsToProduce StartDate StartTime  \n\n# for the more complex example, we are running a timespeedup factor of 5 to push data faster through the system\n\necho \"The producer will loop through the datafile until it finds the selected StartDate.  This can take awhile depending on the date of the month you select.\"\necho \"As an example, starting on the 5th can take 90 seconds before the first data is sent.  Starting on the 10th can take 3 minutes. Starting on the 20th can take 6 minutes.  Starting on the 30th can take 9 minutes.\"\n\n#and I think zeppelin has a default timeout of 10 minutes before it will kill a running shell interpreter\n#if you want to run this for a date in the end of the month, I suggest you make a new datafile that omits earlier data.\n# Example to make a smaller file that starts with Dec 24th. (dec24th starts on the 1366742th line-- seen by looking at the file in vi)\n# head -n 1  bikes/bike_events.csv > bikes/bike_events_startdec24.csv\n# tail -n +1366742 bikes/bike_events.csv >> bikes/bike_events_startdec24.csv\n# python ./citibike_kafka.py bikes/bike_events_startdec24.csv 20 300 2016-12-25 07:00:00 2>&1\n\necho \"..\"\necho \"..\"\necho \"Launching Producer for 2016-12-01 07:00:00 with a time acceleration factor of 5\"\npython ./citibike_kafka.py citibike/bike_streaming_input.csv 5 400 2016-12-01 07:00:00 2>&1\necho \"done\"\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":"false","language":"sh"},"colWidth":6,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379854_-391382313","id":"20170421-172032_481813195","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2332"},{"title":"HTML to display the live map","text":"%angular\n<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.css\" />\n<div id=\"map\" style=\"height: 700px; width: 100%\"></div>\n\n<script type=\"text/javascript\">\n\n\n//based on https://gist.github.com/granturing/a09aed4a302a7367be92, https://community.hortonworks.com/articles/90320/add-leaflet-map-to-zeppelin-notebook.html, etc\nfunction initMap() {\n    //open up a map around NYC at zoom level 13\n    var map = L.map('map', {preferCanvas: true}).setView([40.75, -73.99], 13);\n    //the preferCanvas was needed so that polylines and circles would show right\n\n\n    //define the background tile layer using OpenStreet maps.  Leaflet can work with other providers, too\n    L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n        attribution: 'Map data &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors',\n        maxZoom: 16,\n        minZoom: 11\n    }).addTo(map);\n\n    //define a new LayerGroup which will hold our markers\n    var startMarkers = L.layerGroup().addTo(map);\n    var endMarkers = L.layerGroup().addTo(map);\n\n    var overlayMaps = {\n    \"Top Pickup Stations\": startMarkers,\n    \"Top Dropoff Stations\": endMarkers\n    };\n    \n    //add a Control to the map to let the user click the Marker layer off and on\n    L.control.layers(null, overlayMaps).addTo(map);\n    \n    // keep track of our markers, so we can remove them later\n    var markers = new Array();\n    \n    // setup a custom icon for markers. See https://github.com/pointhi/leaflet-color-markers\n    var greenIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-green.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n    \n    var redIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n\n    var el = angular.element($('#map').parent('.ng-scope'));\n    angular.element(el).ready(function() {\n        \n        //listen for changes to the angular variable called stations\n        window.locationWatcher = el.scope().compiledScope.$watch('bikeevents', function(newValue, oldValue) {\n             //geoMarkers.clearLayers(); -- this did not work for me, so I use the for loop below to delete old markers\n            for(i=0;i<markers.length;i++) {\n               startMarkers.removeLayer(markers[i]);\n               endMarkers.removeLayer(markers[i]);\n               map.removeLayer(markers[i]);\n            }  \n            //now empty our array and start again\n            markers = new Array();\n            \n            //loop through each entry in our stations variable and add it as a marker\n            angular.forEach(newValue, function(bikes) {\n                var marker = L.marker([bikes.lat, bikes.lon], {icon: greenIcon})\n                    .bindPopup(\"<b>\" + bikes.station + \"</b><br>\" + bikes.description)\n                var latlngs = [\n                  [bikes.lat, bikes.lon],\n                  [bikes.midwaylat, bikes.midwaylon]\n                ];\n                \n                if (bikes.eventtype==\"Dropoff\") {\n                    marker.setIcon(redIcon)\n                    marker.addTo(endMarkers)\n                    var polyline = L.polyline(latlngs, {color: 'red'}).addTo(endMarkers);\n                    markers.push(polyline);\n                } else {\n                    marker.addTo(startMarkers);  \n                    var polyline = L.polyline(latlngs, {color: 'green'}).addTo(startMarkers);\n                    markers.push(polyline);\n                }\n                markers.push(marker);   \n                \n                \n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we'll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log('Loading Leaflet library');\n    var sc = document.createElement('script');\n    sc.type = 'text/javascript';\n    sc.src = 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.js';\n    sc.onload = initMap;\n    sc.onerror = function(err) { alert(err); }\n    document.getElementsByTagName('head')[0].appendChild(sc);\n}\n</script>\n\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.css\" />\n<div id=\"map\" style=\"height: 700px; width: 100%\"></div>\n\n<script type=\"text/javascript\">\n\n\n//based on https://gist.github.com/granturing/a09aed4a302a7367be92, https://community.hortonworks.com/articles/90320/add-leaflet-map-to-zeppelin-notebook.html, etc\nfunction initMap() {\n    //open up a map around NYC at zoom level 13\n    var map = L.map('map', {preferCanvas: true}).setView([40.75, -73.99], 13);\n    //the preferCanvas was needed so that polylines and circles would show right\n\n\n    //define the background tile layer using OpenStreet maps.  Leaflet can work with other providers, too\n    L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {\n        attribution: 'Map data &copy; <a href=\"http://openstreetmap.org\">OpenStreetMap</a> contributors',\n        maxZoom: 16,\n        minZoom: 11\n    }).addTo(map);\n\n    //define a new LayerGroup which will hold our markers\n    var startMarkers = L.layerGroup().addTo(map);\n    var endMarkers = L.layerGroup().addTo(map);\n\n    var overlayMaps = {\n    \"Top Pickup Stations\": startMarkers,\n    \"Top Dropoff Stations\": endMarkers\n    };\n    \n    //add a Control to the map to let the user click the Marker layer off and on\n    L.control.layers(null, overlayMaps).addTo(map);\n    \n    // keep track of our markers, so we can remove them later\n    var markers = new Array();\n    \n    // setup a custom icon for markers. See https://github.com/pointhi/leaflet-color-markers\n    var greenIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-green.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n    \n    var redIcon = new L.Icon({\n      iconUrl: 'https://cdn.rawgit.com/pointhi/leaflet-color-markers/master/img/marker-icon-red.png',\n      shadowUrl: 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.7/images/marker-shadow.png',\n      iconSize: [12, 20],\n      iconAnchor: [6, 20],\n      popupAnchor: [1, -17],\n      shadowSize: [20, 20]\n    });\n\n    var el = angular.element($('#map').parent('.ng-scope'));\n    angular.element(el).ready(function() {\n        \n        //listen for changes to the angular variable called stations\n        window.locationWatcher = el.scope().compiledScope.$watch('bikeevents', function(newValue, oldValue) {\n             //geoMarkers.clearLayers(); -- this did not work for me, so I use the for loop below to delete old markers\n            for(i=0;i<markers.length;i++) {\n               startMarkers.removeLayer(markers[i]);\n               endMarkers.removeLayer(markers[i]);\n               map.removeLayer(markers[i]);\n            }  \n            //now empty our array and start again\n            markers = new Array();\n            \n            //loop through each entry in our stations variable and add it as a marker\n            angular.forEach(newValue, function(bikes) {\n                var marker = L.marker([bikes.lat, bikes.lon], {icon: greenIcon})\n                    .bindPopup(\"<b>\" + bikes.station + \"</b><br>\" + bikes.description)\n                var latlngs = [\n                  [bikes.lat, bikes.lon],\n                  [bikes.midwaylat, bikes.midwaylon]\n                ];\n                \n                if (bikes.eventtype==\"Dropoff\") {\n                    marker.setIcon(redIcon)\n                    marker.addTo(endMarkers)\n                    var polyline = L.polyline(latlngs, {color: 'red'}).addTo(endMarkers);\n                    markers.push(polyline);\n                } else {\n                    marker.addTo(startMarkers);  \n                    var polyline = L.polyline(latlngs, {color: 'green'}).addTo(startMarkers);\n                    markers.push(polyline);\n                }\n                markers.push(marker);   \n                \n                \n            });\n        })\n    });\n}\n\nif (window.locationWatcher) {\n    // clear existing watcher otherwise we'll have duplicates\n    window.locationWatcher();\n}\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log('Loading Leaflet library');\n    var sc = document.createElement('script');\n    sc.type = 'text/javascript';\n    sc.src = 'https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.0.3/leaflet.js';\n    sc.onload = initMap;\n    sc.onerror = function(err) { alert(err); }\n    document.getElementsByTagName('head')[0].appendChild(sc);\n}\n</script>"}]},"apps":[],"jobName":"paragraph_1510931379854_-391382313","id":"20170417-195006_1776067804","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2333"},{"text":"%md\n#Next Steps\n\nThis concludes our NYC Citi Bikes demonstration (for now).  \n\n\n\nYou've seen how to:\n + Load Citi Bike data to the Object Store\n + Define a Spark SQL table on the data\n + Run various SQL queries and display the output\n + Show results on a Map\n + Stream bike data and create a live map\n\n\nStay tuned for additional parts of this demonstration coming soon.\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"editorSetting":{"editOnDblClick":"true","language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Next Steps</h1>\n<p>This concludes our NYC Citi Bikes demonstration (for now).</p>\n<p>You've seen how to:</p>\n<ul>\n<li>Load Citi Bike data to the Object Store</li>\n<li>Define a Spark SQL table on the data</li>\n<li>Run various SQL queries and display the output</li>\n<li>Show results on a Map</li>\n<li>Stream bike data and create a live map</li>\n</ul>\n<p>Stay tuned for additional parts of this demonstration coming soon.</p>\n"}]},"apps":[],"jobName":"paragraph_1510931379855_-391767062","id":"20170417-195150_467395081","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2334"},{"text":"%md\n### Change Log\nNovember 16, 2017 - Fixed some minor deprecated Spark calls.  Confirmed it works with 17.4.1\nSeptember 12, 2017 - Confirmed it works with BDCSCE 17.3.5. Fixed an issue with incorrectly generating the file used for streaming input for the producer.\nAugust 23, 2017 - Changed to use scratch directory\nAugust 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20\nAugust 11, 2017 - Journey v2.  Confirmed it works with Spark 2.1\nJuly 28, 2017 - Validated with BDCSCE 17.3.1 and OEHCS 0.10.2\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":"true"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Change Log</h3>\n<p>November 16, 2017 - Fixed some minor deprecated Spark calls. Confirmed it works with 17.4.1<br/>September 12, 2017 - Confirmed it works with BDCSCE 17.3.5. Fixed an issue with incorrectly generating the file used for streaming input for the producer.<br/>August 23, 2017 - Changed to use scratch directory<br/>August 13, 2017 - Confirmed it works with BDCSCE 17.3.3-20<br/>August 11, 2017 - Journey v2. Confirmed it works with Spark 2.1<br/>July 28, 2017 - Validated with BDCSCE 17.3.1 and OEHCS 0.10.2</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510931379855_-391767062","id":"20170503-152817_1702690082","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2335"},{"text":"%md\n","dateUpdated":"2017-11-17T15:09:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":"true"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510931379856_-381378841","id":"20170728-151738_1747139513","dateCreated":"2017-11-17T15:09:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2336"}],"name":"Journeys/New Data Lake/Streaming/Demo/Citi Bike Live Map with Spark Streaming","id":"2CZT2KSAF","angularObjects":{"2CXJR3AHX:shared_process":[],"2CXKU133T:shared_process":[],"2D19V2229:shared_process":[],"2CZAK1K32:shared_process":[],"2D163KRC5:shared_process":[],"2CZVT6YPM:shared_process":[],"2CY7WZWMD:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}